{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Tokenizer, Wav2Vec2ForCTC\n",
    "import librosa as lb\n",
    "import torch\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "C:\\Users\\Brad\\anaconda3\\lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:421: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = Wav2Vec2Tokenizer.from_pretrained('facebook/wav2vec2-base-960h')\n",
    "\n",
    "# Initialize the model\n",
    "model = Wav2Vec2ForCTC.from_pretrained('facebook/wav2vec2-base-960h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1092, -0.1092, -0.1092,  ..., -0.1092, -0.1092, -0.1092]])\n",
      "CausalLMOutput(loss=None, logits=tensor([[[ 13.6210, -28.7729, -28.4780,  ...,  -7.5972,  -7.3417,  -8.0035],\n",
      "         [ 13.5178, -28.7108, -28.4154,  ...,  -7.6053,  -7.2277,  -8.0515],\n",
      "         [ 13.3449, -28.6903, -28.3933,  ...,  -7.6658,  -7.1592,  -8.1156],\n",
      "         ...,\n",
      "         [ 14.5321, -29.3446, -29.0461,  ...,  -7.6393,  -7.5731,  -7.9725],\n",
      "         [ 14.4140, -29.2251, -28.9306,  ...,  -7.5904,  -7.5131,  -7.9238],\n",
      "         [ 14.2885, -29.0835, -28.7909,  ...,  -7.5708,  -7.3951,  -7.9912]]],\n",
      "       grad_fn=<AddBackward0>), hidden_states=None, attentions=None)\n",
      "['HELLO WORLD']\n"
     ]
    }
   ],
   "source": [
    "waveform, rate = lb.load('Hello1.wav', sr = 16000)\n",
    "\n",
    "#0.16 secs to tokenize word\n",
    "input_values = tokenizer(waveform, return_tensors='pt').input_values\n",
    "print(input_values)\n",
    "\n",
    "# Retrieve logits from the model\n",
    "logits = model(input_values).logits\n",
    "print(model(input_values))\n",
    "\n",
    "# Take argmax value and decode into transcription\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = tokenizer.batch_decode(predicted_ids)\n",
    "\n",
    "# Print the output\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "speechRec = pipeline(\"automatic-speech-recognition\", \"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I HOPE YOU ARE DOING WELL DUE TO UNFORESEEN CIRCUMSTANCES WE WILL NOT HAVE CLASS IN PERSON TO MORROW TUESDAY'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, rate = lb.load('Hello2.wav', sr = 16000)\n",
    "speechRec(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
